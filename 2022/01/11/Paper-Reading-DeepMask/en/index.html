<!DOCTYPE html><html lang="zh-CN,zh-TW,en,default"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="Python,AI,Machine Learning,Deep Learning"><meta name="description" content="Python 开发者，致力于人工智能 | 机器学习 | 深度学习 知识&amp;技术分享"><meta name="author" content="Xavier Jiezou"><title>Paper Reading: DeepMask | ZXC&#39;s Blog</title><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="/images/logo@800x800.jpg"><link rel="stylesheet" href="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/css/font-awesome.min.css"><script id="hexo-configurations">let KEEP=window.KEEP||{};KEEP.hexo_config={hostname:"xavierjiezou.github.io",root:"/",language:["zh-CN","zh-TW","en","default"],path:"search.xml"},KEEP.theme_config={toc:{enable:!0,number:!0,expand_all:!0,init_open:!0},style:{primary_color:"#0066CC",avatar:"/images/avatar.jpg",favicon:"/images/logo@800x800.jpg",article_img_align:"left",left_side_width:"260px",content_max_width:"920px",hover:{shadow:!0,scale:!0},first_screen:{enable:!0,background_img:"/images/bg.svg",description:"Keep coding and Keep loving."},scroll:{progress_bar:{enable:!0},percent:{enable:!0}}},local_search:{enable:!0,preload:!0},code_copy:{enable:!0,style:"default"},pjax:{enable:!1},lazyload:{enable:!0},version:"3.4.5"},KEEP.language_ago={second:"%s seconds ago",minute:"%s minutes ago",hour:"%s hours ago",day:"%s days ago",week:"%s weeks ago",month:"%s months ago",year:"%s years ago"}</script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="ZXC's Blog" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-image" href="/"><img src="/images/logo@800x800.jpg"> </a><a class="logo-title" href="/">ZXC&#39;s Blog</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/">首页</a></li><li class="menu-item"><a href="/archives">归档</a></li><li class="menu-item"><a href="/categories">分类</a></li><li class="menu-item"><a href="/tags">标签</a></li><li class="menu-item"><a href="/links">友链</a></li><li class="menu-item"><a href="/about">关于</a></li><li class="menu-item"><a href="/changelog">更新日志</a></li><li class="menu-item search search-popup-trigger"><i class="fas fa-search"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/">首页</a></li><li class="drawer-menu-item flex-center"><a href="/archives">归档</a></li><li class="drawer-menu-item flex-center"><a href="/categories">分类</a></li><li class="drawer-menu-item flex-center"><a href="/tags">标签</a></li><li class="drawer-menu-item flex-center"><a href="/links">友链</a></li><li class="drawer-menu-item flex-center"><a href="/about">关于</a></li><li class="drawer-menu-item flex-center"><a href="/changelog">更新日志</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="article-content-container"><div class="article-title"><span class="title-hover-animation">Paper Reading: DeepMask</span></div><div class="article-header"><div class="avatar"><img src="/images/avatar.jpg"></div><div class="info"><div class="author"><span class="name">Xavier Jiezou</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fas fa-edit"></i>&nbsp; <span class="pc">2022-01-11 20:38:07</span> <span class="mobile">2022-01-11 20:38</span> </span><span class="article-categories article-meta-item"><i class="fas fa-folder"></i>&nbsp;<ul><li><a href="/categories/paper/">paper</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fas fa-tags"></i>&nbsp;<ul><li><a href="/tags/paper/">paper</a>&nbsp;</li><li>| <a href="/tags/deepmask/">deepmask</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fas fa-file-word"></i>&nbsp;<span>1.4k 字</span> </span><span class="article-min2read article-meta-item"><i class="fas fa-clock"></i>&nbsp;<span>7 分钟</span> </span><span class="article-pv article-meta-item"><i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content markdown-body"><p>English | <a href="/2022/01/11/Paper-Reading-DeepMask/zh/">简体中文</a></p><p><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.03607">Deep Mask<i class="fas fa-external-link-alt"></i></a>: an algorithm for cloud and cloud shadow detection in optical satellite remote sensing images using deep residual network</p><hr><h2 id="Abstract">Abstract</h2><p>DeepMask utilizes ResNet, a deep convolutional neural network, for <strong>pixel-level</strong> cloud mask generation.</p><p>The algorithm is trained and evaluated on the <strong>Landsat 8 Cloud Cover Assessment Validation Dataset</strong> distributed across 8 different land types.</p><p>Compared with <strong>CFMask</strong>, the most widely used cloud detection algorithm, land-type-specific DeepMask models achieve higher accuracy across all land types. The average accuracy is <strong>93.56%</strong>, compared with <strong>83.36%</strong> from CFMask.</p><p>Compared with other CNN-based cloud mask algorithm, DeepMask benefits from the <strong>parsimonious architecture</strong> and the residual connection of ResNet. It is compatible with input of any size and shape.</p><h2 id="Introduction">Introduction</h2><p>Studies have found that mean annual global cloud coverage is estimated to be between <strong>58%</strong> and <strong>66%</strong>.</p><p>Manual labeling of cloud and cloud shadow pixels are <strong>expensive</strong> in terms of time and human resources.</p><p>Many cloud mask algorithms have been proposed to tackle this problem. These cloud mask algorithms can be divided into two major categories: <strong>multi-temporal</strong> algorithms and <strong>single-date</strong> algorithms.</p><ul class="lvl-0"><li class="lvl-2"><p>Multi-temporal</p><ul class="lvl-2"><li class="lvl-4">Utilize temporal and statistical information to detect cloud pixels.</li><li class="lvl-4">Computationally expensive: need to process large volume of time series data.</li><li class="lvl-4">Challenging to use: due to the requirement of a clear-sky reference image.</li></ul></li><li class="lvl-2"><p>Single-date</p><ul class="lvl-2"><li class="lvl-4">Threshold-based: physical rules or geometric relationship.</li><li class="lvl-4">Machine learning: fully-connected neural networks, fuzzy models, and svm.</li><li class="lvl-4">Deep learning: super-pixel + CNN, semantic segmentation networks, and CNN + RNN (optical information + temporal information).</li></ul></li></ul><p>CFMask is a multi-stage algorithm that uses a pre-defined set of threshold tests to label pixels, which has the best overall performance on those existing cloud mask algorithms. It further refines cloud mask using statistics. It predicts cloud shadow using cloud height and satellite sensor projection. The CFMask algorithm is currently the most widely used cloud mask algorithm, and it is the default cloud mask algorithm used by Landsat missions.</p><p>CFMask’s weaknesses:</p><ul class="lvl-0"><li class="lvl-2"><p>CFMask tends to confuse clouds with bright targets such as building tops, beaches, snow/ice, sand dunes, and salt lakes.</p></li><li class="lvl-2"><p>Optically thin clouds have a high probability of being omitted by the CFMask algorithm.</p></li></ul><p>Human experts analyze not only the spectral band values of each individual pixel, but also the shape and texture of the local region. This level of context and geospatial information is what CFMask and the other threshold-based approaches lack.</p><p>We evaluate our algorithm on the Landsat 8 CCA dataset, because of its comprehensive human label for cloud and cloud shadow mask and its extensive coverage of different land cover types.</p><p>To understand the contribution of different spectral bands to the cloud mask performance, we also conduct ablation experiments that use a subset of the spectral bands as input.</p><h2 id="Materials-and-Methods">Materials and Methods</h2><h3 id="Landsat-8-Cloud-Cover-Assessment-Validation-Dataset">Landsat 8 Cloud Cover Assessment Validation Dataset</h3><blockquote><p>Landsat 8 Cloud Cover Assessment Validation Data: <a class="link" target="_blank" rel="noopener" href="https://landsat.usgs.gov/landsat-8-cloud-cover-assessment-validation-data">download<i class="fas fa-external-link-alt"></i></a></p></blockquote><h3 id="Data-Preparation">Data Preparation</h3><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642405451463.png" alt="image"></p><p align="center">Figure 1: Data preparation flowchart</p><p>We cast the cloud mask prediction as a binary classification problem. All pixels are divided into 2 classes: clear (ground) pixels and cloud/shadow pixels. We choose not to separate cloud and shadow pixels due to the high <strong>imbalance</strong> between cloud and shadow classes. The entire data preprocessing pipeline is summarized in Figure 1.</p><h3 id="Convolutional-Neural-Network-and-ResNet">Convolutional Neural Network and ResNet</h3><ul class="lvl-0"><li class="lvl-2"><p>CNN</p><ul class="lvl-2"><li class="lvl-4">Be powerful at capturing local features.</li><li class="lvl-4">It has significantly fewer parameters to learn than a fully connected neural network.</li></ul></li><li class="lvl-2"><p>ResNet</p><ul class="lvl-2"><li class="lvl-4">Residual learning: skip connections.</li><li class="lvl-4">ResNets can have much larger depth without the burden of the vanishing or exploding gradient problem.</li></ul></li></ul><h3 id="The-DeepMask-Algorithm">The DeepMask Algorithm</h3><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642406854978.png" alt="image"></p><p>Figure 2: DeepMask algorithm is a unified pipeline, consisting of the local region extractor (module A) and the ResNet backbone (module B). A zoomed-in view (module C) of a typical residual block is also given.</p><h3 id="Experiment-Design">Experiment Design</h3><h4 id="Land-type-specific-Experiments">Land-type-specific Experiments</h4><p>In land-type-specific experiments, we train a model for each land cover type and evaluate its performance on images of that specific land type.</p><p>For example, barren has 12 scene images. One scene image is left out as the test image, and all other 11 images are used for training and validation.</p><h4 id="All-land-type-Experiments">All-land-type Experiments</h4><p>Different from the land-type-specific experiments, all-land-type models are trained using a sample of images from all the land cover types, and then tested on a set of reserved test images. During the training stage, for each land type, we randomly reserve one image as the test image and use all other images for training and validation.</p><h4 id="Ablation-Experiments">Ablation Experiments</h4><p>To analyze the contribution of individual spectral band to cloud and shadow detection accuracy, we also perform ablation experiments on the spectral bands.</p><h3 id="Evaluation-Metrics">Evaluation Metrics</h3><p>Following the tradition in the cloud mask and machine learning literatures, we select overall accuracy, precision, recall, F1 score, Area Under the Receiver Operating Characteristic Curve (AUROC), and Average Precision (AP) as our evaluation metrics.</p><h2 id="Results">Results</h2><h3 id="Quantitative-Results">Quantitative Results</h3><h4 id="Land-type-specific-Experiments-2">Land-type-specific Experiments</h4><p>Table 2: Land-type-specific model test performance averaged across all land types</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642408375990.png" alt="image"></p><p>Table 3: Land-type-specific DeepMask test performance by land type</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642408738605.png" alt="image"></p><p>Table 4: Land-type-specific CFMask test performance by land type</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642408755270.png" alt="image"></p><h4 id="All-land-type-and-Ablation-Experiments">All-land-type and Ablation Experiments</h4><p>Table 5: All land type model test performance and ablation experiments</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642408938610.png" alt="image"></p><h3 id="Qualitative-Results">Qualitative Results</h3><p>Figure 3: Example visualization of raw RGB image (1st row), ground truth labels (2nd row), CFMask results (3rd row), and DeepMask results (last row), for four land types: snow (1st column), water (2nd column), wetland (3rd column) and urban (last column). For ground truth label, CFMask results and DeepMask results, gray color denotes clear pixels, and white color denotes cloud/shadow pixels.</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642409810718.png" alt="image"></p><p>Figure 4: Example visualization of raw image (RGB), ground truth labels, CFMask results, and DeepMask results, for four land types: crops, forest, barren and shrubland. For ground truth label, CFMask results and DeepMask results, gray color denotes clear pixels, and white color denotes cloud/shadow pixels.</p><p><img lazyload src="/images/loading.svg" data-src="https://fastly.jsdelivr.net/gh/XavierJiezou/picx-image-hosting@main/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeepMask/1642409844936.png" alt="image"></p><h2 id="Discussion">Discussion</h2><h2 id="Conclusion">Conclusion</h2><h2 id="Acknowledgment">Acknowledgment</h2><h2 id="References">References</h2><h2 id="Appendix">Appendix</h2><ul class="lvl-0"><li class="lvl-2"><p><strong>pervasive</strong>: a. 普遍的, 流行的, 到处蔓延的, 到处渗透的</p></li></ul><blockquote><p><strong>Example</strong>: Detecting and masking cloud and cloud shadow from satellite remote sensing images is a pervasive problem in the remote sensing community.</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>harness: n. 马具, 挽具状带子, 甲胄 vt. 给…上挽具, 驾驭, 披上甲胄, 利用…以产生动力</p></li><li class="lvl-2"><p><strong>parsimonious</strong>: a. 过度节俭的, 吝啬小气的</p></li><li class="lvl-2"><p><strong>compatible</strong>: a. 能共处的, 可并立的, 适合的 [计] 相容的; 兼容的</p></li><li class="lvl-2"><p>occlusion: n. 闭塞, 咬合, 包藏 [化] 包藏</p></li><li class="lvl-2"><p>contamination: n. 污染, 污物 [化] 污染; 沾污</p></li><li class="lvl-2"><p>ablation experiment: <a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/gaolijing_/article/details/105626733">消融实验<i class="fas fa-external-link-alt"></i></a></p></li><li class="lvl-2"><p>biome: n. 生物群落区</p></li><li class="lvl-2"><p>altitude: n. 高度, 海拔, 高处 [电] 高度</p></li><li class="lvl-2"><p>heterogeneity: n. 异种, 异质, 不同成分 [化] 不均匀性; 多相性</p></li><li class="lvl-2"><p>atmospheric correction: 大气矫正</p></li><li class="lvl-2"><p>land type classification: 土地类型分类</p></li><li class="lvl-2"><p>tackle: n. 工具, 复滑车, 滑车, 装备, 扭倒 vt. 固定, 处理, 抓住 vi. 扭倒</p></li><li class="lvl-2"><p><strong>contaminated</strong>: a. 受污染的；弄脏的</p></li><li class="lvl-2"><p>derive: vt. 得自 vi. 起源</p></li><li class="lvl-2"><p>with the renaissance of computer vision: 随着计算机视觉的复兴</p></li><li class="lvl-2"><p>comprehensive: a. 广泛的, 有理解力的, 综合的 [经] 广泛的, 综合的, 全面的</p></li><li class="lvl-2"><p>backbone: n. 脊椎, 志气, 骨干, 支柱 [计] 主干网, 主干网点</p></li><li class="lvl-2"><p>pre-processing or post-processing: 预处理或后处理</p></li><li class="lvl-2"><p>convention: n. 大会, 协定, 惯例, 约定 [计] 约定</p></li></ul><blockquote><p><strong>Example</strong>: We compare our performance against CFMask, following the convention of most cloud mask papers.</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>fraction: n. 小部分, 破片, 分数 [医] 部分, 成分, 分散</p></li><li class="lvl-2"><p>annotate: v. 作注解, 注释 [计] 注释</p></li><li class="lvl-2"><p>slight: n. 轻蔑, 怠慢 a. 轻微的, 纤细的, 脆弱的, 苗条的 vt. 轻视, 忽略, 怠慢</p></li><li class="lvl-2"><p>intersection: n. 交集, 十字路口, 交叉点 [计] 逻辑乘; 与</p></li><li class="lvl-2"><p>cast: n. 演员阵容, 投掷, 铸件, 预测, 特性 vt. 投, 掷, 抛, 脱落, 铸, 使弯曲, 计算 vi. 投, 计算, 浇铸成型</p></li><li class="lvl-2"></li></ul></div><div class="post-copyright-info"><div class="article-copyright-info-container"><ul><li>本文标题：Paper Reading: DeepMask</li><li>本文作者：Xavier Jiezou</li><li>创建时间：2022-01-11 20:38:07</li><li>本文链接：https://github.ghgxj.xyz/2022/01/11/Paper-Reading-DeepMask/en/</li><li>版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></div><ul class="post-tags-box"><li class="tag-item"><a href="/tags/paper/">#paper</a>&nbsp;</li><li class="tag-item"><a href="/tags/deepmask/">#deepmask</a>&nbsp;</li></ul><div class="article-nav"><div class="article-prev"><a class="prev" rel="prev" href="/2022/01/11/Paper-Reading-DeepMask/zh/"><span class="left arrow-icon flex-center"><i class="fas fa-chevron-left"></i> </span><span class="title flex-center"><span class="post-nav-title-item">论文阅读：DeepMask</span> <span class="post-nav-item">上一篇</span></span></a></div><div class="article-next"><a class="next" rel="next" href="/2022/01/10/Blog-v1-0-1-Released/"><span class="title flex-center"><span class="post-nav-title-item">Blog v1.0.1 Released</span> <span class="post-nav-item">下一篇</span> </span><span class="right arrow-icon flex-center"><i class="fas fa-chevron-right"></i></span></a></div></div><div class="comment-container"><div class="comments-container"><div id="comment-anchor"></div><div class="comment-area-title"><i class="fas fa-comments">&nbsp;评论</i></div><div class="twikoo-container"><script src="//fastly.jsdelivr.net/npm/twikoo@1.0.0/dist/twikoo.all.min.js"></script><div id="twikoo-comment"></div><script>function loadTwikoo(){twikoo.init({el:"#twikoo-comment",envId:"twikoo-5glasdyj56858c6d",region:""})}{const o=setTimeout(()=>{loadTwikoo(),clearTimeout(o)},1e3)}</script></div></div></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2021</span> - 2022&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Xavier Jiezou</a></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item"><span id="busuanzi_container_site_uv">访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp; </span><span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span></div><div class="theme-info info-item">由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a></div><div class="icp-info info-item"><a target="_blank" rel="nofollow" href="https://beian.miit.gov.cn">鄂ICP备2021017368号</a></div></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="tools-list"><li class="tools-item page-aside-toggle"><i class="fas fa-outdent"></i></li><li class="go-comment"><i class="fas fa-comment"></i></li></ul></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fas fa-search-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fas fa-search-minus"></i></li><li class="tools-item tool-expand-width flex-center"><i class="fas fa-arrows-alt-h"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fas fa-moon"></i></li><li class="tools-item rss flex-center"><a class="flex-center" href="/atom.xml" target="_blank"><i class="fas fa-rss"></i></a></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fas fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fas fa-cog fa-spin"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><aside class="page-aside"><div class="post-toc-wrap"><div class="post-toc"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Materials-and-Methods"><span class="nav-number">3.</span> <span class="nav-text">Materials and Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Landsat-8-Cloud-Cover-Assessment-Validation-Dataset"><span class="nav-number">3.1.</span> <span class="nav-text">Landsat 8 Cloud Cover Assessment Validation Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Preparation"><span class="nav-number">3.2.</span> <span class="nav-text">Data Preparation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolutional-Neural-Network-and-ResNet"><span class="nav-number">3.3.</span> <span class="nav-text">Convolutional Neural Network and ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-DeepMask-Algorithm"><span class="nav-number">3.4.</span> <span class="nav-text">The DeepMask Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiment-Design"><span class="nav-number">3.5.</span> <span class="nav-text">Experiment Design</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Land-type-specific-Experiments"><span class="nav-number">3.5.1.</span> <span class="nav-text">Land-type-specific Experiments</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#All-land-type-Experiments"><span class="nav-number">3.5.2.</span> <span class="nav-text">All-land-type Experiments</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ablation-Experiments"><span class="nav-number">3.5.3.</span> <span class="nav-text">Ablation Experiments</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-Metrics"><span class="nav-number">3.6.</span> <span class="nav-text">Evaluation Metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Results"><span class="nav-number">4.</span> <span class="nav-text">Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantitative-Results"><span class="nav-number">4.1.</span> <span class="nav-text">Quantitative Results</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Land-type-specific-Experiments-2"><span class="nav-number">4.1.1.</span> <span class="nav-text">Land-type-specific Experiments</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#All-land-type-and-Ablation-Experiments"><span class="nav-number">4.1.2.</span> <span class="nav-text">All-land-type and Ablation Experiments</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qualitative-Results"><span class="nav-number">4.2.</span> <span class="nav-text">Qualitative Results</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Discussion"><span class="nav-number">5.</span> <span class="nav-text">Discussion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">6.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Acknowledgment"><span class="nav-number">7.</span> <span class="nav-text">Acknowledgment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">8.</span> <span class="nav-text">References</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix"><span class="nav-number">9.</span> <span class="nav-text">Appendix</span></a></li></ol></div></div></aside><div class="image-viewer-container"><img src=""></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fas fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fas fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/utils.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/main.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/header-shrink.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/back2top.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/dark-light-toggle.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/local-search.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/code-copy.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/lazyload.js"></script><div class="post-scripts"><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/left-side-toggle.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/libs/anime.min.js"></script><script src="//fastly.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/toc.js"></script></div></body></html>